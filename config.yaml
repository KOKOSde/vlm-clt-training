# Example config for CLT training
# Update paths to match your environment

model_id: "your-model-id"  # e.g., "llava-hf/llava-1.5-7b-hf", "Qwen/Qwen2-VL-7B"

# Layers to train (adjust based on your model's architecture)
layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]

# Training feature dimension (typically 2x hidden_dim)
feature_dim: 8192

# Sequence and batching
seq_len: 512
batch_size: 1
grad_accum_steps: 8

# Data paths (update these for your setup)
data:
  train_jsonl: "./data/train.jsonl"
  val_jsonl: "./data/val.jsonl"
  test_jsonl: "./data/test.jsonl"

# Output directories
outputs:
  activations_dir: "./activations"
  transcoders_dir: "./checkpoints"
  logs_dir: "./logs"
